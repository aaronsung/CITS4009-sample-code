#' ---
#' title: "Sampling"
#' subtitle: "CITS4009 Computational Data Analysis"
#' author: "A/Prof Wei Liu"
#' institute: |
#'     | Department of Computer Science and Software Engineering
#'     | The University of Western Australia
#' graphics: yes
#' date: "Semester 2, 2023"
#' 
#' output:
#'   beamer_presentation:
#'     theme: "AnnArbor"
#'     colortheme: "dolphin"
#'     fonttheme: "structurebold"
#'     includes:
#'         in_header: "../common.tex"
#' ---
#' 
## ----setup, include=FALSE------------------------------------------------------------
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", message=FALSE, warning=FALSE)
library(knitr)
library(ggplot2)
library(crayon)
custdata <- read.table('../../data/custdata.tsv', header=T, sep='\t')

#' 
#' ## Why Sampling?
#' 
#' With today's computer power, we can analyze very large datasets than
#' before, but sampling is a necessary task for other reasons also. 
#' 
#' - It is easier to test and debug the code on small subsamples before training the model on the entire dataset.
#' 
#' - Visualization can be easier with a subsample of the data; 
#'     + `ggplot` runs faster on smaller datasets, and too much data can often obscure the patterns in a graph. 
#' 
#' - To train a model, we often need to split the entire dataset into a \stress{test} (or \stress{hold-out}) set and a \stress{training} set. 
#'     + The training set is the data that you feed to your model-building algorithm (regression, decision trees, and so on).
#'     + The test set is the data that you feed into the resultant trained model, to verify that its predictions are accurate.
#' 
#' ## Random Samples
#' 
#' The `sample()` function allows us to take a random sample (with or without replacement) of size `n` from the elements
#' in `x` (usually a vector).
#' 
#' \footnotesize
## ------------------------------------------------------------------------------------
rows <- sample(1:nrow(custdata), 3, replace=FALSE)
mysample <- custdata[rows, ]

# show the first 6 columns of mysample
mysample[, 1:6]

#' \normalsize
#' 
#' 
#' ## Sampling with or without replacement
#' 
#' - Sampling **with replacement** &ndash; one data point can be drawn multiple times, because the drawn data points are *replaced* or put back into the population again.
#'   
#'     + This means all the data points have the same probability of being sampled in \underline{each} draw.
#'   
#' - Sampling **without replacement** &ndash; once the data point is drawn, it is no longer available for future selection.
#' 
#'     + This means the remaining data points have the same but a higher probability of being sampled in \underline{each later} draw.
#' 
#' - When the population is large enough, sampling without replacement is not much different from sampling with replacement.   
#' 
#' 
#' ## Test and training splits (reproducible sampling)
#' 
#' One way to manage random sampling is to add a *sample group* column.
#' 
#' - The *sample group* column contains numbers generated uniformly from zero to one, using the `runif` function.
#' 
#' \footnotesize
## ------------------------------------------------------------------------------------
# create a new column
custdata$gp <- runif(dim(custdata)[1])

#' \normalsize
#' 
#' - This makes the samples reproducible as compared to the built-in `sample()` function. 
#'   That is, we use the `custdata$gp` column again and again whenever we want to
#'   extract a test set and a training set.
#' 
#' \footnotesize
## ----collapse=T----------------------------------------------------------------------
split.ratio <- 0.1
testSet <- subset(custdata, custdata$gp <= split.ratio)
trainingSet <- subset(custdata, custdata$gp > split.ratio)
cat("Test set size:", dim(testSet)[1])
cat("Training set size:", dim(trainingSet)[1])

#' \normalsize
#' 
#' ## Test and training splits (random sampling)
#' 
#' If we want to test how robust a model (e.g., a *decision trees classifier*) is for
#' different subsets of observations in our dataset, then we want the test and
#' training sets to be different in each trial.  In such a case, we can modify the
#' code above as follows:
#' 
#' \footnotesize
## ----collapse=T----------------------------------------------------------------------
s <- runif(dim(custdata)[1])
split.ratio <- 0.1
testSet <- subset(custdata, s <= split.ratio)
trainingSet <- subset(custdata, s > split.ratio)
cat("Test set size:", dim(testSet)[1])
cat("Training set size:", dim(trainingSet)[1])

#' \normalsize
#' 
#' That is, in each trial, we generate the random numbers from `runif()` again.
#' 
#' ## Random seed
#' 
#' Same as in other programming languages, random numbers in R are pseudorandom
#' numbers generated by a *random number generator* (an algorithm). See the examples given in
#' https://r-coder.com/set-seed-r/
#' 
#' We can set the \stressi{random seed} to initialize the random number generator.
#' 
#' - If we set the *seed* to a fixed number (any integer), e.g., `set.seed(5)`, then we get the same sequence of pseudorandom numbers generated each time.
#' 
#' - If we want a different sequence of pseudorandom numbers generated each time, the best solution is set the seed to the system time, e.g., `set.seed(Sys.time())` or simply `set.seed(NULL)`.  Type `?set.seed` in R to see the details.
#' 
#' 
#' ##   Take home messages
#' 
#' - Random Sampling
#' 
#' - Reproducible Sampling
#' 
#' 
#' ##   References
#' 
#' Below are the sections for further reading for all the topics in this week's lecture:
#' 
#' - __Practical Data Science with R__, _Nina Zumel, John Mount_, Manning, 2nd Ed., 2020 (Chapter 4 - Sections 4.3.1-4.3.2; Chapter 5: Sections 5.1-5.4)
#' 
#' 
#' - **R for Data Science**, *Hadley Wickham, Garrett Grolemund*, [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/) (Chapters 11,12,13,18; Section 27.2 (for the `%>%` pipe and R markdown))
